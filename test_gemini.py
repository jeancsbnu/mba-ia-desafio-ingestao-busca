import os
from dotenv import load_dotenv
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_postgres import PGVector
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Carregar vari√°veis do ambiente
load_dotenv()

def test_gemini_model():
    """Testar especificamente o modelo Gemini 2.5 Flash Lite"""
    
    # Configura√ß√µes
    pgvector_url = os.getenv("PGVECTOR_URL", "postgresql://postgres:postgres@localhost:5432/rag")
    collection_name = os.getenv("PGVECTOR_COLLECTION", "documents")
    google_api_key = os.getenv("GOOGLE_API_KEY")
    
    print("ü§ñ Inicializando embeddings locais...")
    
    # Embeddings locais
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    print("üîå Conectando ao banco vetorial...")
    
    # Banco vetorial
    store = PGVector(
        embeddings=embeddings,
        connection=pgvector_url,
        collection_name=collection_name,
        use_jsonb=True
    )
    
    # Retriever
    retriever = store.as_retriever(search_kwargs={"k": 3})
    
    print("üß† Testando Google Gemini 2.5 Flash Lite...")
    
    # LLM (Google Gemini)
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        google_api_key=google_api_key,
        temperature=0.3
    )
    
    # Template do prompt mais espec√≠fico
    template = """
Com base no contexto fornecido, responda √† pergunta de forma clara e objetiva.

CONTEXTO:
{context}

PERGUNTA: {question}

INSTRU√á√ïES:
- Use apenas as informa√ß√µes do contexto fornecido
- Se voc√™ conseguir identificar informa√ß√µes relevantes no contexto, forne√ßa uma resposta baseada nelas
- Seja espec√≠fico e cite dados quando relevantes
- Use portugu√™s brasileiro

RESPOSTA:"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
    
    # Criar cadeia RAG
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt}
    )
    
    # Testes espec√≠ficos
    perguntas = [
        "Quais s√£o algumas empresas listadas no documento?",
        "Que tipo de informa√ß√µes o documento cont√©m sobre as empresas?",
        "Qual √© o ano de funda√ß√£o mais antigo mencionado?",
        "Existem valores em reais no documento?"
    ]
    
    print("\n" + "="*60)
    print("üî¨ TESTE DO MODELO GEMINI 2.5 FLASH LITE")
    print("="*60)
    
    for i, pergunta in enumerate(perguntas, 1):
        print(f"\n{i}. üìù Pergunta: {pergunta}")
        print("   üîç Processando...")
        
        try:
            resposta = qa_chain.invoke(pergunta)
            print(f"   ü§ñ Resposta: {resposta['result']}")
        except Exception as e:
            print(f"   ‚ùå Erro: {e}")
        
        print("   " + "-"*50)

if __name__ == "__main__":
    test_gemini_model()