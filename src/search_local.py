import os
from dotenv import load_dotenv
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_postgres import PGVector
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Carregar vari√°veis do ambiente
load_dotenv()

def search_documents(query: str, top_k: int = 5) -> list:
    """Buscar documentos similares no banco vetorial"""
    
    # Configura√ß√µes
    pgvector_url = os.getenv("PGVECTOR_URL", "postgresql://postgres:postgres@localhost:5432/rag")
    collection_name = os.getenv("PGVECTOR_COLLECTION", "documents")
    google_api_key = os.getenv("GOOGLE_API_KEY")
    
    # Usar embeddings locais (mesmo modelo da ingest√£o)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # Conectar ao banco vetorial
    store = PGVector(
        embeddings=embeddings,
        connection=pgvector_url,
        collection_name=collection_name,
        use_jsonb=True
    )
    
    # Buscar documentos similares
    results = store.similarity_search_with_score(query, k=top_k)
    
    return results

def create_rag_chain():
    """Criar cadeia RAG com Google Gemini e embeddings locais"""
    
    # Configura√ß√µes
    pgvector_url = os.getenv("PGVECTOR_URL", "postgresql://postgres:postgres@localhost:5432/rag")
    collection_name = os.getenv("PGVECTOR_COLLECTION", "documents")
    google_api_key = os.getenv("GOOGLE_API_KEY")
    
    # Embeddings locais
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # Banco vetorial
    store = PGVector(
        embeddings=embeddings,
        connection=pgvector_url,
        collection_name=collection_name,
        use_jsonb=True
    )
    
    # Retriever
    retriever = store.as_retriever(search_kwargs={"k": 5})
    
    # LLM (Google Gemini)
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        google_api_key=google_api_key,
        temperature=0.1
    )
    
    # Template do prompt
    template = """
Voc√™ √© um assistente especializado em responder perguntas baseadas nos documentos fornecidos.

Contexto dos documentos:
{context}

Pergunta: {question}

Instru√ß√µes:
- Responda baseado APENAS no contexto fornecido
- Se a informa√ß√£o n√£o estiver nos documentos, diga "N√£o encontrei essa informa√ß√£o nos documentos"
- Seja preciso e cite trechos relevantes quando poss√≠vel
- Use portugu√™s brasileiro

Resposta:"""

    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )
    
    # Criar cadeia RAG
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt}
    )
    
    return qa_chain

def main():
    print("üîç Sistema de Busca RAG - Teste")
    print("=" * 50)
    
    # Teste de busca simples
    query = "What is the main topic of this document?"
    print(f"\nüìù Consulta: {query}")
    
    results = search_documents(query)
    
    print(f"\nüìö Encontrados {len(results)} documentos similares:")
    for i, (doc, score) in enumerate(results, 1):
        print(f"\n{i}. Score: {score:.4f}")
        print(f"   Conte√∫do: {doc.page_content[:200]}...")
        print(f"   Metadados: {doc.metadata}")
    
    # Teste da cadeia RAG
    print("\n" + "=" * 50)
    print("ü§ñ Teste da Cadeia RAG")
    
    qa_chain = create_rag_chain()
    response = qa_chain.run(query)
    
    print(f"\n‚ùì Pergunta: {query}")
    print(f"\nüí¨ Resposta: {response}")

if __name__ == "__main__":
    main()