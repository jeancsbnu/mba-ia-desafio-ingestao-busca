# Sistema RAG - IngestÃ£o e Busca SemÃ¢ntica

Sistema de ingestÃ£o e busca semÃ¢ntica usando LangChain, PostgreSQL com pgVector e embeddings HuggingFace.

## ğŸ“‹ Funcionalidades

- **IngestÃ£o**: Processa arquivos PDF e armazena chunks no banco vetorial
- **Busca**: Responde perguntas baseadas exclusivamente no conteÃºdo do PDF
- **CLI**: Interface de linha de comando simples e direta

## ğŸ› ï¸ Tecnologias

- **Python** - Linguagem principal
- **LangChain** - Framework para aplicaÃ§Ãµes RAG
- **PostgreSQL + pgVector** - Banco vetorial
- **HuggingFace Embeddings** - Embeddings locais (sentence-transformers)
- **Google Gemini 2.5 Flash Lite** - Modelo de linguagem
- **Docker & Docker Compose** - ExecuÃ§Ã£o do banco

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ docker-compose.yml          # ConfiguraÃ§Ã£o PostgreSQL + pgVector
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â”œâ”€â”€ .env.example              # Template de variÃ¡veis de ambiente
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py             # Script de ingestÃ£o do PDF
â”‚   â”œâ”€â”€ search.py             # FunÃ§Ãµes de busca vetorial
â”‚   â”œâ”€â”€ chat.py               # Interface CLI
â”œâ”€â”€ document.pdf              # PDF para ingestÃ£o
â””â”€â”€ README.md                 # Este arquivo
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. Clonar repositÃ³rio
```bash
git clone <repository-url>
cd mba-ia-desafio-ingestao-busca
```

### 2. Configurar ambiente virtual
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

### 3. Instalar dependÃªncias
```bash
pip install -r requirements.txt
```

### 4. Configurar variÃ¡veis de ambiente
```bash
cp .env.example .env
```

Edite o arquivo `.env` com suas credenciais:
```bash
GOOGLE_API_KEY=sua-chave-google-aqui
PGVECTOR_URL=postgresql://postgres:postgres@localhost:5432/rag
PGVECTOR_COLLECTION=documents
PDF_PATH=document.pdf
```

## ğŸš€ ExecuÃ§Ã£o

### 1. Iniciar banco de dados
```bash
docker compose up -d
```

### 2. Executar ingestÃ£o do PDF
```bash
python src/ingest.py
```

### 3. Iniciar chat
```bash
python src/chat.py
```

## ğŸ’¬ Exemplo de Uso

```
FaÃ§a sua pergunta:

PERGUNTA: Qual o faturamento da empresa Magna Financeira Holding?
RESPOSTA: R$ 51.046.000,25

---

FaÃ§a sua pergunta:

PERGUNTA: Quantos clientes temos em 2024?
RESPOSTA: NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.
```

## ğŸ”§ EspecificaÃ§Ãµes TÃ©cnicas

### IngestÃ£o
- **Chunk size**: 1000 caracteres
- **Overlap**: 150 caracteres
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2 (local)
- **Splitter**: RecursiveCharacterTextSplitter

### Busca
- **MÃ©todo**: similarity_search_with_score
- **Resultados**: k=10 documentos mais relevantes
- **LLM**: Google Gemini 2.5 Flash Lite

### Prompt Template
O sistema usa um prompt rigoroso que:
- Responde apenas com base no contexto fornecido
- Retorna "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias" para perguntas fora do contexto
- Nunca inventa informaÃ§Ãµes ou usa conhecimento externo

## ğŸ§ª Testes

Para testar o sistema:
```bash
python test_chat.py
```

## ğŸ“Š Arquivos de ConfiguraÃ§Ã£o

### docker-compose.yml
Configura PostgreSQL com extensÃ£o pgVector para armazenamento vetorial.

### .env.example
Template com todas as variÃ¡veis de ambiente necessÃ¡rias.

### requirements.txt
Lista completa de dependÃªncias Python incluindo:
- langchain
- langchain-community  
- langchain-postgres
- langchain-google-genai
- sentence-transformers
- E outras dependÃªncias necessÃ¡rias

## âš ï¸ Requisitos

- Python 3.8+
- Docker e Docker Compose
- Chave de API do Google Gemini
- Pelo menos 2GB de RAM livres
- ConexÃ£o com internet (para download inicial dos modelos)

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro de conexÃ£o com banco
```bash
docker compose down
docker compose up -d
```

### Erro de modelo nÃ£o encontrado
Verifique se a chave do Google API estÃ¡ correta no arquivo `.env`.

### Erro de dependÃªncias
```bash
pip install --upgrade -r requirements.txt
```

## ğŸ“ Notas

- Os embeddings sÃ£o processados localmente (HuggingFace)
- Apenas o LLM utiliza API externa (Google Gemini)
- O sistema funciona offline apÃ³s download inicial dos modelos
- Os dados sÃ£o persistidos no PostgreSQL via Docker

---

**Desenvolvido com LangChain, PostgreSQL, pgVector e HuggingFace**